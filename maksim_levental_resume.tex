%% start of file `template.tex'.
%% Copyright 2006-2013 Xavier Danaux (xdanaux@gmail.com).
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License version 1.3c,
% available at http://www.latex-project.org/lppl/.


\documentclass[11pt,letterpaper,roman,colorlinks,linkcolor=blue]{moderncv}

% modern themes
\moderncvstyle{banking}  % style options are 'casual' (default), 'classic', 'oldstyle' and 'banking'
\moderncvcolor{grey}     % color options 'blue' , 'orange', 'green', 'red', 'purple', 'grey' and 'black'
\nopagenumbers{}

\usepackage{enumitem}
\usepackage{hang}
\usepackage[left=0.5in, top=0.5in, right=0.5in, bottom=0.5in]{geometry}
\usepackage{import}
\usepackage{xcolor}

\setlist[itemize]{topsep=0pt,itemsep=0pt,parsep=0pt,before=\vspace{1.5pt},after=\vspace{4pt}, label=\raisebox{0.25ex}{\tiny$\bullet$}, leftmargin=0.3in}
\setlength{\hangingindent}{0.4in}

\definecolor{defaultext}{RGB}{55, 65, 92}
\makeatletter
\newcommand{\globalcolor}[1]{%
  \color{#1}\global\let\default@color\current@color
}
\makeatother

\AtBeginDocument{\globalcolor{darkgray}}

\newcommand*{\modern}{\fontfamily{qhv}\selectfont}
\definecolor{mygrey}{RGB}{55, 65, 92} %{104, 104, 104}
\newcommand{\mystyle}[1]{\textcolor{mygrey}{\modern #1}}
\newcommand{\mytitlestyle}[1]{\huge\mystyle{#1}}
\newcommand{\mysectionstyle}[1]{\large\mystyle{#1}}


% personal data
\name{\mytitlestyle{Maksim}}{\mytitlestyle{Levental}}
% \address{415 Lovett Blvd. Houston, TX 77006}{}{}
% \phone[mobile]{}
% \email{}
% \homepage{}
% \extrainfo{}

%----------------------------------------------------------------------------------
%            content
%----------------------------------------------------------------------------------

\begin{document}
\makecvtitle

\vspace{-16mm}
\begin{center}
\mystyle{\href{maksim.levental@gmail.com}{\texttt{maksim.levental@gmail}} $|$ \href{https://makslevental.github.io/}{\texttt{makslevental.github.io}} $|$ \href{https://github.com/makslevental}{\texttt{github.com/makslevental}}}
\end{center}

%----------------------------------------------------------------------------------------
%	INTERESTS SECTION
%----------------------------------------------------------------------------------------

\section{\mysectionstyle{INTERESTS}}

I'm interested in using, improving, and designing application-specific hardware accelerators for Deep Neural Networks (DNNs), for science applications. 
Specifically, using compiler techniques such as symbolic execution, and mathematical optimization techniques such as integer programming, to improve memory consumption and inference latencies of DNNs on GPUs, FPGAs, and ASICs. 

%----------------------------------------------------------------------------------------
%	EDUCATION SECTION
%----------------------------------------------------------------------------------------

\section{\mysectionstyle{EDUCATION}}

\textbf{Ph.D.}, \textbf{Computer Science}, \emph{University of Chicago}, GPA: 4.0
\hfill \textbf{Oct. 2020 --- Present}

\textbf{M.S.}, \textbf{Computer Science}, \emph{University of Chicago}, GPA: 4.0
\hfill \textbf{Aug. 2019 --- Feb. 2022}

\textbf{M.S.}, \textbf{Computer Science}, \emph{University of Florida}, GPA: 3.7
\hfill \textbf{Aug. 2014 --- Dec. 2015}

\textbf{B.S.}, \textbf{Pure Math}, \textbf{Honors}, \emph{Florida State University}, GPA: 3.7
\hfill \textbf{Aug. 2007 --- Dec. 2010}

%----------------------------------------------------------------------------------------
%	EXPERIENCE SECTION
%----------------------------------------------------------------------------------------

\section{\mysectionstyle{EXPERIENCE}}

\textbf{Graduate Research Assistant}, Globus Labs, \emph{University of Chicago}
\hfill \textbf{Oct. 2020 --- Present}
\begin{itemize}
\item Building an end-to-end open-source flow from PyTorch to FPGA, using MLIR, LLVM, and Vitis HLS. Currently exploring affine loop transformations.
\item Investigated a design workflow that allows exploring algorithmically complex hardware designs and developing reusable hardware libraries for the needs of scientific instruments at the edge. Specifically, investigated Chisel and other HDLs for designing DNNs and compression algorithms.
\item Helped develop a workflow that connects the Data and Learning Hub for Science (DLHub), a repository for publishing AI models, with the Hardware-Accelerated Learning (HAL) cluster, using funcX as a universal distributed computing service. Specifically, deployed Autoencoder models for identifying binary black hole mergers in LIGO data.
\item Developed a hybrid CPU-GPU Difference of Gaussians method for characterizing the volume distribution of unburnt nanoparticle synthesis solution, to enable near-real-time optimization and steering of Flame Spray Pyrolysis experiments.\end{itemize}

\textbf{Distributed Systems Research Intern}, PyTorch, \emph{Facebook}
\hfill \textbf{June 2022 --- Sept. 2022}
\begin{itemize}
\item Researched automated tensor sharding for distributed training of large models.
\item Studied DP formulation of inter-node communication (\texttt{alpa}'s) overhead and contributed a $\sim$200x improvement in performance for large tensors.
\item Used MLIR to explore statically inferring tensor sharding and compute paralellization strategies (based on shape analysis/refinement).
\end{itemize}

\textbf{Compiler Research Intern}, Torch-MLIR, \emph{nod.ai}
\hfill \textbf{Feb. 2022 --- June 2022, Sept. 2022 --- Present}
\begin{itemize}
\item Currently building an end-to-end compiler for PyTorch models (through Torch-MLIR) which functions independently of PyTorch (\href{https://github.com/nod-ai/PI}{https://github.com/nod-ai/PI}).
\item Built out first implementation of eager-mode for Torch-MLIR using \texttt{torch\_dispatch} and JITing techniques.
\item Implemented various new ``ops'' and infrastructure (including, extending C bindings).
\end{itemize}

\textbf{Compiler Research Intern}, PyTorch, \emph{Facebook}
\hfill \textbf{June 2021 --- Feb. 2022}
\begin{itemize}
\item Developed functionality for statically allocating memory for reduced inference latency in production and OSS PyTorch models.
\item Developed symbolic analysis techniques for inferring memory requirements of intermediate tensors in dynamic neural networks. Specifically, used and extended shape analysis in TorchScript, combined with MIP formulation of storage allocation, to derive upper bounds on sizes of intermediate tensors.  
\end{itemize}

\textbf{Teaching Assistant}, Computer Science Department, \emph{University of Chicago}
\hfill \textbf{Apr. 2021 --- June 2021}
\begin{itemize}
\item Assisted with grading, designed assignments, and hosted discussions for MPCS 52040 - Distributed Systems. Specifically, contributed example implementation of RAFT consensus in Rust.
\end{itemize}

\textbf{Ranking Engineer Intern}, Groups, \emph{Facebook}
\hfill \textbf{June 2020 --- Sep. 2020}
\begin{itemize}
\item Investigated effects of repeated recommendations on conversion for Groups You Should Join (GYSJ). Specifically, developed and shipped pipelines and metrics for measuring repetition. 
\item Developed features used in scoring recommendations, including collecting, cleaning, and transforming data.
\item Ran A/B tests on 100MM population to measure efficacy of aforementioned features in reducing repetition and increasing conversion. Effects were marginal (i.e., not statistically significant).
\end{itemize}

\textbf{Graduate Research Assistant}, \emph{University of Florida}
\hfill \textbf{Aug. 2014 --- Dec. 2015, Aug. 2018 --- Dec. 2019}
\begin{itemize}
\item Investigated classical and DNN Super Resolution techniques for improving object detection in Long Wave Infrared imagery. Specifically, improved detection AUC for very low-resolution targets by 5\% (using SRGAN).
\item Developed a novel training paradigm using, region-based stratified cross-validation, to improve learning induction across disparate (in terms of geography) landmine data sets.
\item Researched using Gaussian Processes (GP) for landmine detection. Specifically, modeling soil permittivity/dielectric changes as GPs.
\item Implemented Postgres backend for landmine detection algorithm/testing/scoring framework (TUF).
\end{itemize}

\textbf{Peace Corps Education Volunteer}, \emph{Mbale, Uganda}
\hfill \textbf{March 2011 --- March 2013}
\begin{itemize}
\item Taught O level and A level math classes to rural school children. Ran study sessions for national exams.
\item Taught IT classes to students and teachers on standard software suites (Office, Creative Suite, etc.). Worked on initiative to bring WiFi to the school campus.
\item Taught classes on life skills, science at girls (GLOW) and boys (BUILD) empowerment camps.
\end{itemize}


%----------------------------------------------------------------------------------------
% TECHINICAL STRENGTHS	
%----------------------------------------------------------------------------------------

\section{\mysectionstyle{SKILLS}}

\begin{tabular}{ @{} >{\bfseries}l @{\hspace{6ex}} l }
Formal Languages  & Python, C++, Rust, SQL, CUDA, Scala \\
Human Languages  & English (native), Russian (native) \\
Platforms  & LLVM, MLIR, PyTorch, TensorFlow, PostgreSQL \\
Skills     & Deep Learning, Compilers, Hardware, Baking \\
Coursework & \textit{Computer Science}: Quantum Computing, Deep Learning Systems, \\
	   & $\qquad$ $\qquad$ $\qquad$ $\quad$ Analysis of Algorithms, Consensus and Economics, Automata \\
           & \textit{Math}: Statistics, Numerical Linear Algebra, Optimization, Real Analysis, Topology, \\
           & $\qquad$ \hspace{0.5ex} Algebra, PDEs  \\
           & \textit{Physics}: Computational Physics, Electricity and Magnetism, Quantum Mechanics, \\ 
           & $\qquad$ $\quad$ \hspace{0.00ex} Statistical Mechanics, Nuclear Physics, Waves and Optics
\end{tabular}

%----------------------------------------------------------------------------------------
%	PUBLICATIONS SECTION
%----------------------------------------------------------------------------------------

\section{\mysectionstyle{PUBLICATIONS}}


\begin{hangingpar}
\textbf{Levental M.}, Khan A., Chard R., Yoshi K., Chard K., Foster I.,
\textit{OpenHLS: High-Level Synthesis for Low-Latency Deep Neural Networks for Experimental Science} 
\textbf{Submitted}.
\end{hangingpar}

\begin{hangingpar}
\textbf{Levental M.}, Chard R., Chard K., Foster I., Wildenberg G.,
\textit{Ultrafast Focus Detection for Automated Microscopy} 
eScience IEEE 17th International Conference on eScience (2021).
\end{hangingpar}

% \begin{hangingpar}
% \textbf{Levental M.}, Bao B., Chard K., Foster I.,
% \textit{MemoMalloc: Memory Planning for Deep Neural Networks.} 
% \textbf{Submitted} to 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI '22).
% \end{hangingpar}

\begin{hangingpar}
Huerta E., Khan A., Huang X., Tian M., \textbf{Levental M.}, Chard R., Wei W., Heflin M., Katz D., Kindratenko V., Mu D., Blaiszik B., Foster I.,
\textit{Accelerated, Scalable and Reproducible AI-driven Gravitational Wave Detection.} 
Nature Astronomy volume 5, pages 1062-1068 (2021).
\end{hangingpar}

\begin{hangingpar}
Yoshii K., Sankaran R., Strempfer S., \textbf{Levental M.}, Hammer M., Miceli A.,
\textit{A Hardware Co-design Workflow for Scientific Instruments at the Edge.} 
Accepted to the Smoky Mountains Computational Sciences and Engineering Conference (SMC '21).
\end{hangingpar}

\begin{hangingpar}
\textbf{Levental M.}, Chard R., Libera J. A., Chard K., Koripelly A., Elias J., Schwarting M., Blaiszik B., Stan M., Chaudhuri S., Foster I.,
\textit{Towards Online Steering of Flame Spray Pyrolysis Nanoparticle Synthesis.} 
\textbf{Best Paper at 2020 IEEE/ACM 2nd Annual Workshop on Extreme-scale Experiment-in-the-Loop Computing (XLOOP '20)}.
\end{hangingpar}

\begin{hangingpar}
Wilson J., Toska F., \textbf{Levental M.}, Dobbins P.,
\textit{A Deep Neural Network Model for Hazard Classification.}
Artificial Intelligence and Machine Learning in Defense Applications (2019).
\end{hangingpar}

\section{\mysectionstyle{TECHNICAL REPORTS}}

\begin{hangingpar}
\textbf{Levental M.},
\textit{MS Thesis: Memory Planning for Deep Neural Networks} 
\href{https://arxiv.org/submit/4178464/view}{arXiv:4178464} (2022).
\end{hangingpar}

\begin{hangingpar}
\textbf{Levental M.},
\textit{Tensor Networks for Simulating Quantum Circuits on FPGAs.} 
\href{https://arxiv.org/abs/2108.06831}{arXiv:2108.06831} (2021).
\end{hangingpar}

\begin{hangingpar}
\textbf{Levental M.}, Orlova E.
\textit{Comparing the Costs of Abstraction for DL Frameworks.} 
\href{https://arxiv.org/abs/2012.07163}{arXiv:2012.07163} (2020).
\end{hangingpar}

%\begin{hangingpar}
%Zhang, Z., Huang, Lei., \textbf{Pauloski, J. G.}, Foster, I. T. (2019, November).
%\textit{FanStore: Enabling Efficient and Scalable I/O for Distributed Deep Learning.}
%IEEE/ACM Third Workshop on Deep Learning on Supercomputers, Denver, Colorado.
%\end{hangingpar}
%
%\begin{hangingpar}
%\textbf{Pauloski, J. Gregory.} (2018, September).
%\textit{Optimizing Deep Learning Methods for Image Segmentation with Distributed Training.}
%Poster presented at the TACC Symposium for Texas Researchers, Austin, TX.
%\end{hangingpar}
%
%\begin{hangingpar}
%Gates, E., \textbf{Pauloski, J. G.}, Schellingerhout, D., Fuentes, D. (2018, September).
%\textit{Glioma Segmentation and a Simple Accurate Model for Overall Survival Prediction.}
%In International MICCAI Brainlesion Workshop (pp. 476-484). Springer, Cham.
%\end{hangingpar}
%
%\begin{hangingpar}
%\textbf{Pauloski, J. Gregory.} (2017, March). 
%\textit{The Cultural Offensive Against ISIS.}
%Received a UT Undergraduate Writing Award.
%\end{hangingpar}

\end{document}
